CUDA_VISIBLE_DEVICES=
Thu Sep  4 18:42:37 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 575.57.08              Driver Version: 575.57.08      CUDA Version: 12.9     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Quadro RTX 6000                Off |   00000000:3B:00.0 Off |                  Off |
| 34%   28C    P8              6W /  260W |       1MiB /  24576MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
world size: 1, rank: 0, local rank: 0
{
  "SHELL": "/bin/bash",
  "SLURM_STEP_NUM_TASKS": "1",
  "SLURM_JOB_USER": "ashmal",
  "SLURM_TASKS_PER_NODE": "1",
  "SLURM_JOB_UID": "1045",
  "NVM_INC": "/home/ashmal/.nvm/versions/node/v20.19.3/include/node",
  "SLURM_STEP_GPUS": "1",
  "CONDA_MKL_INTERFACE_LAYER_BACKUP": "",
  "SLURM_TASK_PID": "56594",
  "CONDA_EXE": "/home/ashmal/anaconda3/bin/conda",
  "_CE_M": "",
  "LOCAL_RANK": "0",
  "SLURM_JOB_GPUS": "0",
  "SLURM_LOCALID": "0",
  "SLURM_SUBMIT_DIR": "/home/ashmal/Courses/CVS/Assignment_2/scripts",
  "HOSTNAME": "c3-6",
  "LANGUAGE": "en_US:",
  "SLURMD_NODENAME": "c3-6",
  "MASTER_PORT": "29500",
  "SLURM_JOB_START_TIME": "1757025756",
  "HYDRA_LAUNCHER_EXTRA_ARGS": "--external-launcher",
  "SLURM_STEP_NODELIST": "c3-6",
  "SLURM_CPUS_PER_GPU": "8",
  "SLURM_CLUSTER_NAME": "crcv",
  "SLURM_JOB_END_TIME": "1759617756",
  "SLURM_CPUS_ON_NODE": "12",
  "SLURM_JOB_CPUS_PER_NODE": "12",
  "SLURM_GPUS_ON_NODE": "1",
  "PRTE_MCA_plm_slurm_args": "--external-launcher",
  "PWD": "/home/ashmal/Courses/CVS/Assignment_2/scripts/DINO",
  "SLURM_GTIDS": "0",
  "LOGNAME": "ashmal",
  "XDG_SESSION_TYPE": "tty",
  "CONDA_PREFIX": "/home/ashmal/anaconda3/envs/cvs_ass2_dino",
  "SLURM_JOB_PARTITION": "gpu",
  "MODULESHOME": "/usr/share/modules",
  "JUPYTER_SERVER_URL": "http://login2:8880/",
  "SLURM_TRES_PER_TASK": "cpu=12",
  "ROCR_VISIBLE_DEVICES": "0",
  "SLURM_OOM_KILL_STEP": "0",
  "SLURM_JOB_NUM_NODES": "1",
  "SRUN_DEBUG": "3",
  "SLURM_STEPID": "0",
  "SLURM_JOBID": "289699",
  "WORLD_SIZE": "1",
  "SLURM_PTY_PORT": "46151",
  "I_MPI_HYDRA_BOOTSTRAP_EXEC_EXTRA_ARGS": "--external-launcher",
  "SLURM_JOB_QOS": "group1-1",
  "SLURM_LAUNCH_NODE_IPADDR": "192.168.136.203",
  "LINES": "58",
  "HOME": "/home/ashmal",
  "SLURM_PTY_WIN_ROW": "58",
  "LANG": "en_US",
  "LS_COLORS": "rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=00:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.avif=01;35:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.webp=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:*~=00;90:*#=00;90:*.bak=00;90:*.crdownload=00;90:*.dpkg-dist=00;90:*.dpkg-new=00;90:*.dpkg-old=00;90:*.dpkg-tmp=00;90:*.old=00;90:*.orig=00;90:*.part=00;90:*.rej=00;90:*.rpmnew=00;90:*.rpmorig=00;90:*.rpmsave=00;90:*.swp=00;90:*.tmp=00;90:*.ucf-dist=00;90:*.ucf-new=00;90:*.ucf-old=00;90:",
  "SLURMD_DEBUG": "2",
  "SLURM_PROCID": "0",
  "COLUMNS": "192",
  "PYTHONSTARTUP": "/etc/slurm/scripts/prompt.py",
  "CONDA_PROMPT_MODIFIER": "(cvs_ass2_dino) ",
  "TMPDIR": "/tmp",
  "PYDEVD_USE_FRAME_EVAL": "NO",
  "SLURM_CPUS_PER_TASK": "12",
  "SLURM_NTASKS": "1",
  "SLURM_TOPOLOGY_ADDR": "c3-6",
  "SSH_CONNECTION": "10.32.219.171 52896 10.173.204.135 22",
  "ZE_AFFINITY_MASK": "0",
  "HYDRA_BOOTSTRAP": "slurm",
  "NVM_DIR": "/home/ashmal/.nvm",
  "MASTER_ADDR": "127.0.0.1",
  "CUDA_VISIBLE_DEVICES": "0",
  "SLURM_TOPOLOGY_ADDR_PATTERN": "node",
  "SLURM_SRUN_COMM_HOST": "192.168.136.203",
  "LESSCLOSE": "/usr/bin/lesspipe %s %s",
  "XDG_SESSION_CLASS": "user",
  "SLURM_SCRIPT_CONTEXT": "prolog_task",
  "SLURM_MEM_PER_NODE": "49152",
  "JUPYTER_SERVER_ROOT": "/home/ashmal/Courses/CVS",
  "TERM": "xterm-256color",
  "_CE_CONDA": "",
  "LESSOPEN": "| /usr/bin/lesspipe %s",
  "SLURM_PTY_WIN_COL": "192",
  "USER": "ashmal",
  "SLURM_NODELIST": "c3-6",
  "ENVIRONMENT": "BATCH",
  "GPU_DEVICE_ORDINAL": "0",
  "CONDA_SHLVL": "2",
  "SLURM_SRUN_COMM_PORT": "42445",
  "LOADEDMODULES": "",
  "SLURM_STEP_ID": "0",
  "SLURM_JOB_ACCOUNT": "group1",
  "SLURM_PRIO_PROCESS": "0",
  "SLURM_NPROCS": "1",
  "SHLVL": "5",
  "NVM_CD_FLAGS": "",
  "SLURM_NNODES": "1",
  "PYXTERM_DIMENSIONS": "80x25",
  "XDG_SESSION_ID": "3102",
  "SLURM_SUBMIT_HOST": "c3-6",
  "CONDA_PYTHON_EXE": "/home/ashmal/anaconda3/bin/python",
  "XDG_RUNTIME_DIR": "/run/user/1045",
  "SLURM_JOB_ID": "289699",
  "RANK": "0",
  "SLURM_NODEID": "0",
  "SLURM_STEP_NUM_NODES": "1",
  "SSH_CLIENT": "10.32.219.171 52896 22",
  "CONDA_DEFAULT_ENV": "cvs_ass2_dino",
  "__MODULES_LMINIT": "module use --append /etc/environment-modules/modules:module use --append /usr/share/modules/versions:module use --append /usr/share/modules/$MODULE_VERSION/modulefiles:module use --append /usr/share/modules/modulefiles:module use --append /share/apps/Modules",
  "DEBUGINFOD_URLS": "https://debuginfod.ubuntu.com ",
  "SLURM_STEP_TASKS_PER_NODE": "1",
  "TMOUT": "300",
  "SLURM_JOB_LICENSES": "srun:1",
  "SLURM_CONF": "/var/spool/slurm/slurmd/conf-cache/slurm.conf",
  "PATH": "/home/ashmal/.local/bin:/home/ashmal/.nvm/versions/node/v20.19.3/bin:/home/ashmal/google-cloud-sdk/bin:/home/ashmal/anaconda3/envs/cvs_ass2_dino/bin:/home/ashmal/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/opt/slurm/bin",
  "SLURM_JOB_NAME": "DETR",
  "MODULEPATH": "/etc/environment-modules/modules:/usr/share/modules/versions:/usr/share/modules/$MODULE_VERSION/modulefiles:/usr/share/modules/modulefiles:/share/apps/Modules",
  "DBUS_SESSION_BUS_ADDRESS": "unix:path=/run/user/1045/bus",
  "NVM_BIN": "/home/ashmal/.nvm/versions/node/v20.19.3/bin",
  "SSH_TTY": "/dev/pts/33",
  "SLURM_JOB_GROUP": "ashmal",
  "CONDA_PREFIX_1": "/home/ashmal/anaconda3",
  "OMPI_MCA_plm_slurm_args": "--external-launcher",
  "SLURM_STEP_LAUNCHER_PORT": "42445",
  "SLURM_JOB_GID": "1045",
  "OLDPWD": "/home/ashmal/Courses/CVS/Assignment_2/scripts",
  "SLURM_JOB_NODELIST": "c3-6",
  "MODULES_CMD": "/usr/lib/x86_64-linux-gnu/modulecmd.tcl",
  "MKL_INTERFACE_LAYER": "LP64,GNU",
  "I_MPI_HYDRA_BOOTSTRAP": "slurm",
  "BASH_FUNC_ml%%": "() {  module ml \"$@\"\n}",
  "BASH_FUNC_module%%": "() {  local _mlredir=1;\n if [ -n \"${MODULES_REDIRECT_OUTPUT+x}\" ]; then\n if [ \"$MODULES_REDIRECT_OUTPUT\" = '0' ]; then\n _mlredir=0;\n else\n if [ \"$MODULES_REDIRECT_OUTPUT\" = '1' ]; then\n _mlredir=1;\n fi;\n fi;\n fi;\n case \" $@ \" in \n *' --no-redirect '*)\n _mlredir=0\n ;;\n *' --redirect '*)\n _mlredir=1\n ;;\n esac;\n if [ $_mlredir -eq 0 ]; then\n _module_raw \"$@\";\n else\n _module_raw \"$@\" 2>&1;\n fi\n}",
  "BASH_FUNC__module_raw%%": "() {  eval \"$(/usr/bin/tclsh8.6 '/usr/lib/x86_64-linux-gnu/modulecmd.tcl' bash \"$@\")\";\n _mlstatus=$?;\n return $_mlstatus\n}",
  "_": "/home/ashmal/anaconda3/envs/cvs_ass2_dino/bin/python",
  "QT_QPA_PLATFORM_PLUGIN_PATH": "/home/ashmal/anaconda3/envs/cvs_ass2_dino/lib/python3.7/site-packages/cv2/qt/plugins",
  "QT_QPA_FONTDIR": "/home/ashmal/anaconda3/envs/cvs_ass2_dino/lib/python3.7/site-packages/cv2/qt/fonts",
  "LD_LIBRARY_PATH": "/home/ashmal/anaconda3/envs/cvs_ass2_dino/lib/python3.7/site-packages/cv2/../../lib64:"
}
world_size:1 rank:0 local_rank:0
| distributed init (rank 0): env://
Before torch.distributed.barrier()
End torch.distributed.barrier()
Loading config file from config/DINO/DINO_4scale.py
[09/04 18:42:43.378]: git:
  sha: d84a491d41898b3befd8294d1cf2614661fc0953, status: has uncommited changes, branch: main

[09/04 18:42:43.379]: Command: main.py --output_dir logs/DINO/R50-MS4-%j -c config/DINO/DINO_4scale.py --coco_path /home/ashmal/Courses/CVS/Assignment_2/data/2/coco2017 --eval --resume Weights/checkpoint0011_4scale.pth --options dn_scalar=100 embed_init_tgt=TRUE dn_label_coef=1.0 dn_bbox_coef=1.0 use_ema=False dn_box_noise_scale=1.0
[09/04 18:42:43.382]: Full config saved to logs/DINO/R50-MS4-%j/config_args_all.json
[09/04 18:42:43.382]: world size: 1
[09/04 18:42:43.382]: rank: 0
[09/04 18:42:43.382]: local_rank: 0
[09/04 18:42:43.382]: args: Namespace(add_channel_attention=False, add_pos_value=False, amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5.0, box_attn_type='roi_align', clip_max_norm=0.1, cls_loss_coef=1.0, coco_panoptic_path=None, coco_path='/home/ashmal/Courses/CVS/Assignment_2/data/2/coco2017', config_file='config/DINO/DINO_4scale.py', dabdetr_deformable_decoder=False, dabdetr_deformable_encoder=False, dabdetr_yolo_like_anchor_update=False, data_aug_max_size=1333, data_aug_scale_overlap=None, data_aug_scales=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], data_aug_scales2_crop=[384, 600], data_aug_scales2_resize=[400, 500, 600], dataset_file='coco', ddetr_lr_param=False, debug=False, dec_layer_number=None, dec_layers=6, dec_n_points=4, dec_pred_bbox_embed_share=True, dec_pred_class_embed_share=True, decoder_layer_noise=False, decoder_module_seq=['sa', 'ca', 'ffn'], decoder_sa_type='sa', device='cuda', dice_loss_coef=1.0, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, dln_hw_noise=0.2, dln_xy_noise=0.2, dn_bbox_coef=1.0, dn_box_noise_scale=1.0, dn_label_coef=1.0, dn_label_noise_ratio=0.5, dn_labelbook_size=91, dn_number=100, dn_scalar=100, dropout=0.0, ema_decay=0.9997, ema_epoch=0, embed_init_tgt=True, enc_layers=6, enc_loss_coef=1.0, enc_n_points=4, epochs=12, eval=True, find_unused_params=False, finetune_ignore=None, fix_refpoints_hw=-1, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2.0, gpu=0, hidden_dim=256, interm_loss_coef=1.0, local_rank=0, lr=0.0001, lr_backbone=1e-05, lr_backbone_names=['backbone.0'], lr_drop=11, lr_drop_list=[33, 45], lr_linear_proj_mult=0.1, lr_linear_proj_names=['reference_points', 'sampling_offsets'], mask_loss_coef=1.0, masks=False, match_unstable_error=True, matcher_type='HungarianMatcher', modelname='dino', multi_step_lr=False, nheads=8, nms_iou_threshold=-1, no_interm_box_loss=False, note='', num_classes=91, num_feature_levels=4, num_patterns=0, num_queries=900, num_select=300, num_workers=10, onecyclelr=False, options={'dn_scalar': 100, 'embed_init_tgt': True, 'dn_label_coef': 1.0, 'dn_bbox_coef': 1.0, 'use_ema': False, 'dn_box_noise_scale': 1.0}, output_dir='logs/DINO/R50-MS4-%j', param_dict_type='default', pdetr3_bbox_embed_diff_each_layer=False, pdetr3_refHW=-1, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, query_dim=4, random_refpoints_xy=False, rank=0, remove_difficult=False, resume='Weights/checkpoint0011_4scale.pth', return_interm_indices=[1, 2, 3], save_checkpoint_interval=1, save_log=False, save_results=False, seed=42, set_cost_bbox=5.0, set_cost_class=2.0, set_cost_giou=2.0, start_epoch=0, test=False, transformer_activation='relu', two_stage_add_query_num=0, two_stage_bbox_embed_share=False, two_stage_class_embed_share=False, two_stage_default_hw=0.05, two_stage_keep_all_tokens=False, two_stage_learn_wh=False, two_stage_pat_embed=0, two_stage_type='standard', unic_layers=0, use_checkpoint=False, use_deformable_box_attn=False, use_detached_boxes_dec_out=False, use_dn=True, use_ema=False, weight_decay=0.0001, world_size=1)

Namespace(add_channel_attention=False, add_pos_value=False, amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5.0, box_attn_type='roi_align', clip_max_norm=0.1, cls_loss_coef=1.0, coco_panoptic_path=None, coco_path='/home/ashmal/Courses/CVS/Assignment_2/data/2/coco2017', config_file='config/DINO/DINO_4scale.py', dabdetr_deformable_decoder=False, dabdetr_deformable_encoder=False, dabdetr_yolo_like_anchor_update=False, data_aug_max_size=1333, data_aug_scale_overlap=None, data_aug_scales=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], data_aug_scales2_crop=[384, 600], data_aug_scales2_resize=[400, 500, 600], dataset_file='coco', ddetr_lr_param=False, debug=False, dec_layer_number=None, dec_layers=6, dec_n_points=4, dec_pred_bbox_embed_share=True, dec_pred_class_embed_share=True, decoder_layer_noise=False, decoder_module_seq=['sa', 'ca', 'ffn'], decoder_sa_type='sa', device='cuda', dice_loss_coef=1.0, dilation=False, dim_feedforward=2048, dist_backend='nccl', dist_url='env://', distributed=True, dln_hw_noise=0.2, dln_xy_noise=0.2, dn_bbox_coef=1.0, dn_box_noise_scale=1.0, dn_label_coef=1.0, dn_label_noise_ratio=0.5, dn_labelbook_size=91, dn_number=100, dn_scalar=100, dropout=0.0, ema_decay=0.9997, ema_epoch=0, embed_init_tgt=True, enc_layers=6, enc_loss_coef=1.0, enc_n_points=4, epochs=12, eval=True, find_unused_params=False, finetune_ignore=None, fix_refpoints_hw=-1, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2.0, gpu=0, hidden_dim=256, interm_loss_coef=1.0, local_rank=0, lr=0.0001, lr_backbone=1e-05, lr_backbone_names=['backbone.0'], lr_drop=11, lr_drop_list=[33, 45], lr_linear_proj_mult=0.1, lr_linear_proj_names=['reference_points', 'sampling_offsets'], mask_loss_coef=1.0, masks=False, match_unstable_error=True, matcher_type='HungarianMatcher', modelname='dino', multi_step_lr=False, nheads=8, nms_iou_threshold=-1, no_interm_box_loss=False, note='', num_classes=91, num_feature_levels=4, num_patterns=0, num_queries=900, num_select=300, num_workers=10, onecyclelr=False, options={'dn_scalar': 100, 'embed_init_tgt': True, 'dn_label_coef': 1.0, 'dn_bbox_coef': 1.0, 'use_ema': False, 'dn_box_noise_scale': 1.0}, output_dir='logs/DINO/R50-MS4-%j', param_dict_type='default', pdetr3_bbox_embed_diff_each_layer=False, pdetr3_refHW=-1, pe_temperatureH=20, pe_temperatureW=20, position_embedding='sine', pre_norm=False, pretrain_model_path=None, query_dim=4, random_refpoints_xy=False, rank=0, remove_difficult=False, resume='Weights/checkpoint0011_4scale.pth', return_interm_indices=[1, 2, 3], save_checkpoint_interval=1, save_log=False, save_results=False, seed=42, set_cost_bbox=5.0, set_cost_class=2.0, set_cost_giou=2.0, start_epoch=0, test=False, transformer_activation='relu', two_stage_add_query_num=0, two_stage_bbox_embed_share=False, two_stage_class_embed_share=False, two_stage_default_hw=0.05, two_stage_keep_all_tokens=False, two_stage_learn_wh=False, two_stage_pat_embed=0, two_stage_type='standard', unic_layers=0, use_checkpoint=False, use_deformable_box_attn=False, use_detached_boxes_dec_out=False, use_dn=True, use_ema=False, weight_decay=0.0001, world_size=1)
[09/04 18:42:45.130]: number of params:46670782
[09/04 18:42:45.131]: params:
{
  "module.transformer.level_embed": 1024,
  "module.transformer.encoder.layers.0.self_attn.sampling_offsets.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.sampling_offsets.bias": 256,
  "module.transformer.encoder.layers.0.self_attn.attention_weights.weight": 32768,
  "module.transformer.encoder.layers.0.self_attn.attention_weights.bias": 128,
  "module.transformer.encoder.layers.0.self_attn.value_proj.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.value_proj.bias": 256,
  "module.transformer.encoder.layers.0.self_attn.output_proj.weight": 65536,
  "module.transformer.encoder.layers.0.self_attn.output_proj.bias": 256,
  "module.transformer.encoder.layers.0.norm1.weight": 256,
  "module.transformer.encoder.layers.0.norm1.bias": 256,
  "module.transformer.encoder.layers.0.linear1.weight": 524288,
  "module.transformer.encoder.layers.0.linear1.bias": 2048,
  "module.transformer.encoder.layers.0.linear2.weight": 524288,
  "module.transformer.encoder.layers.0.linear2.bias": 256,
  "module.transformer.encoder.layers.0.norm2.weight": 256,
  "module.transformer.encoder.layers.0.norm2.bias": 256,
  "module.transformer.encoder.layers.1.self_attn.sampling_offsets.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.sampling_offsets.bias": 256,
  "module.transformer.encoder.layers.1.self_attn.attention_weights.weight": 32768,
  "module.transformer.encoder.layers.1.self_attn.attention_weights.bias": 128,
  "module.transformer.encoder.layers.1.self_attn.value_proj.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.value_proj.bias": 256,
  "module.transformer.encoder.layers.1.self_attn.output_proj.weight": 65536,
  "module.transformer.encoder.layers.1.self_attn.output_proj.bias": 256,
  "module.transformer.encoder.layers.1.norm1.weight": 256,
  "module.transformer.encoder.layers.1.norm1.bias": 256,
  "module.transformer.encoder.layers.1.linear1.weight": 524288,
  "module.transformer.encoder.layers.1.linear1.bias": 2048,
  "module.transformer.encoder.layers.1.linear2.weight": 524288,
  "module.transformer.encoder.layers.1.linear2.bias": 256,
  "module.transformer.encoder.layers.1.norm2.weight": 256,
  "module.transformer.encoder.layers.1.norm2.bias": 256,
  "module.transformer.encoder.layers.2.self_attn.sampling_offsets.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.sampling_offsets.bias": 256,
  "module.transformer.encoder.layers.2.self_attn.attention_weights.weight": 32768,
  "module.transformer.encoder.layers.2.self_attn.attention_weights.bias": 128,
  "module.transformer.encoder.layers.2.self_attn.value_proj.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.value_proj.bias": 256,
  "module.transformer.encoder.layers.2.self_attn.output_proj.weight": 65536,
  "module.transformer.encoder.layers.2.self_attn.output_proj.bias": 256,
  "module.transformer.encoder.layers.2.norm1.weight": 256,
  "module.transformer.encoder.layers.2.norm1.bias": 256,
  "module.transformer.encoder.layers.2.linear1.weight": 524288,
  "module.transformer.encoder.layers.2.linear1.bias": 2048,
  "module.transformer.encoder.layers.2.linear2.weight": 524288,
  "module.transformer.encoder.layers.2.linear2.bias": 256,
  "module.transformer.encoder.layers.2.norm2.weight": 256,
  "module.transformer.encoder.layers.2.norm2.bias": 256,
  "module.transformer.encoder.layers.3.self_attn.sampling_offsets.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.sampling_offsets.bias": 256,
  "module.transformer.encoder.layers.3.self_attn.attention_weights.weight": 32768,
  "module.transformer.encoder.layers.3.self_attn.attention_weights.bias": 128,
  "module.transformer.encoder.layers.3.self_attn.value_proj.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.value_proj.bias": 256,
  "module.transformer.encoder.layers.3.self_attn.output_proj.weight": 65536,
  "module.transformer.encoder.layers.3.self_attn.output_proj.bias": 256,
  "module.transformer.encoder.layers.3.norm1.weight": 256,
  "module.transformer.encoder.layers.3.norm1.bias": 256,
  "module.transformer.encoder.layers.3.linear1.weight": 524288,
  "module.transformer.encoder.layers.3.linear1.bias": 2048,
  "module.transformer.encoder.layers.3.linear2.weight": 524288,
  "module.transformer.encoder.layers.3.linear2.bias": 256,
  "module.transformer.encoder.layers.3.norm2.weight": 256,
  "module.transformer.encoder.layers.3.norm2.bias": 256,
  "module.transformer.encoder.layers.4.self_attn.sampling_offsets.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.sampling_offsets.bias": 256,
  "module.transformer.encoder.layers.4.self_attn.attention_weights.weight": 32768,
  "module.transformer.encoder.layers.4.self_attn.attention_weights.bias": 128,
  "module.transformer.encoder.layers.4.self_attn.value_proj.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.value_proj.bias": 256,
  "module.transformer.encoder.layers.4.self_attn.output_proj.weight": 65536,
  "module.transformer.encoder.layers.4.self_attn.output_proj.bias": 256,
  "module.transformer.encoder.layers.4.norm1.weight": 256,
  "module.transformer.encoder.layers.4.norm1.bias": 256,
  "module.transformer.encoder.layers.4.linear1.weight": 524288,
  "module.transformer.encoder.layers.4.linear1.bias": 2048,
  "module.transformer.encoder.layers.4.linear2.weight": 524288,
  "module.transformer.encoder.layers.4.linear2.bias": 256,
  "module.transformer.encoder.layers.4.norm2.weight": 256,
  "module.transformer.encoder.layers.4.norm2.bias": 256,
  "module.transformer.encoder.layers.5.self_attn.sampling_offsets.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.sampling_offsets.bias": 256,
  "module.transformer.encoder.layers.5.self_attn.attention_weights.weight": 32768,
  "module.transformer.encoder.layers.5.self_attn.attention_weights.bias": 128,
  "module.transformer.encoder.layers.5.self_attn.value_proj.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.value_proj.bias": 256,
  "module.transformer.encoder.layers.5.self_attn.output_proj.weight": 65536,
  "module.transformer.encoder.layers.5.self_attn.output_proj.bias": 256,
  "module.transformer.encoder.layers.5.norm1.weight": 256,
  "module.transformer.encoder.layers.5.norm1.bias": 256,
  "module.transformer.encoder.layers.5.linear1.weight": 524288,
  "module.transformer.encoder.layers.5.linear1.bias": 2048,
  "module.transformer.encoder.layers.5.linear2.weight": 524288,
  "module.transformer.encoder.layers.5.linear2.bias": 256,
  "module.transformer.encoder.layers.5.norm2.weight": 256,
  "module.transformer.encoder.layers.5.norm2.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.sampling_offsets.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.weight": 32768,
  "module.transformer.decoder.layers.0.cross_attn.attention_weights.bias": 128,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.0.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm1.weight": 256,
  "module.transformer.decoder.layers.0.norm1.bias": 256,
  "module.transformer.decoder.layers.0.self_attn.in_proj_weight": 196608,
  "module.transformer.decoder.layers.0.self_attn.in_proj_bias": 768,
  "module.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.0.norm2.weight": 256,
  "module.transformer.decoder.layers.0.norm2.bias": 256,
  "module.transformer.decoder.layers.0.linear1.weight": 524288,
  "module.transformer.decoder.layers.0.linear1.bias": 2048,
  "module.transformer.decoder.layers.0.linear2.weight": 524288,
  "module.transformer.decoder.layers.0.linear2.bias": 256,
  "module.transformer.decoder.layers.0.norm3.weight": 256,
  "module.transformer.decoder.layers.0.norm3.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.sampling_offsets.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.weight": 32768,
  "module.transformer.decoder.layers.1.cross_attn.attention_weights.bias": 128,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.1.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm1.weight": 256,
  "module.transformer.decoder.layers.1.norm1.bias": 256,
  "module.transformer.decoder.layers.1.self_attn.in_proj_weight": 196608,
  "module.transformer.decoder.layers.1.self_attn.in_proj_bias": 768,
  "module.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.1.norm2.weight": 256,
  "module.transformer.decoder.layers.1.norm2.bias": 256,
  "module.transformer.decoder.layers.1.linear1.weight": 524288,
  "module.transformer.decoder.layers.1.linear1.bias": 2048,
  "module.transformer.decoder.layers.1.linear2.weight": 524288,
  "module.transformer.decoder.layers.1.linear2.bias": 256,
  "module.transformer.decoder.layers.1.norm3.weight": 256,
  "module.transformer.decoder.layers.1.norm3.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.sampling_offsets.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.weight": 32768,
  "module.transformer.decoder.layers.2.cross_attn.attention_weights.bias": 128,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.2.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm1.weight": 256,
  "module.transformer.decoder.layers.2.norm1.bias": 256,
  "module.transformer.decoder.layers.2.self_attn.in_proj_weight": 196608,
  "module.transformer.decoder.layers.2.self_attn.in_proj_bias": 768,
  "module.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.2.norm2.weight": 256,
  "module.transformer.decoder.layers.2.norm2.bias": 256,
  "module.transformer.decoder.layers.2.linear1.weight": 524288,
  "module.transformer.decoder.layers.2.linear1.bias": 2048,
  "module.transformer.decoder.layers.2.linear2.weight": 524288,
  "module.transformer.decoder.layers.2.linear2.bias": 256,
  "module.transformer.decoder.layers.2.norm3.weight": 256,
  "module.transformer.decoder.layers.2.norm3.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.sampling_offsets.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.weight": 32768,
  "module.transformer.decoder.layers.3.cross_attn.attention_weights.bias": 128,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.3.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm1.weight": 256,
  "module.transformer.decoder.layers.3.norm1.bias": 256,
  "module.transformer.decoder.layers.3.self_attn.in_proj_weight": 196608,
  "module.transformer.decoder.layers.3.self_attn.in_proj_bias": 768,
  "module.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.3.norm2.weight": 256,
  "module.transformer.decoder.layers.3.norm2.bias": 256,
  "module.transformer.decoder.layers.3.linear1.weight": 524288,
  "module.transformer.decoder.layers.3.linear1.bias": 2048,
  "module.transformer.decoder.layers.3.linear2.weight": 524288,
  "module.transformer.decoder.layers.3.linear2.bias": 256,
  "module.transformer.decoder.layers.3.norm3.weight": 256,
  "module.transformer.decoder.layers.3.norm3.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.sampling_offsets.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.weight": 32768,
  "module.transformer.decoder.layers.4.cross_attn.attention_weights.bias": 128,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.4.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm1.weight": 256,
  "module.transformer.decoder.layers.4.norm1.bias": 256,
  "module.transformer.decoder.layers.4.self_attn.in_proj_weight": 196608,
  "module.transformer.decoder.layers.4.self_attn.in_proj_bias": 768,
  "module.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.4.norm2.weight": 256,
  "module.transformer.decoder.layers.4.norm2.bias": 256,
  "module.transformer.decoder.layers.4.linear1.weight": 524288,
  "module.transformer.decoder.layers.4.linear1.bias": 2048,
  "module.transformer.decoder.layers.4.linear2.weight": 524288,
  "module.transformer.decoder.layers.4.linear2.bias": 256,
  "module.transformer.decoder.layers.4.norm3.weight": 256,
  "module.transformer.decoder.layers.4.norm3.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.sampling_offsets.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.weight": 32768,
  "module.transformer.decoder.layers.5.cross_attn.attention_weights.bias": 128,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.value_proj.bias": 256,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.weight": 65536,
  "module.transformer.decoder.layers.5.cross_attn.output_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm1.weight": 256,
  "module.transformer.decoder.layers.5.norm1.bias": 256,
  "module.transformer.decoder.layers.5.self_attn.in_proj_weight": 196608,
  "module.transformer.decoder.layers.5.self_attn.in_proj_bias": 768,
  "module.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "module.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "module.transformer.decoder.layers.5.norm2.weight": 256,
  "module.transformer.decoder.layers.5.norm2.bias": 256,
  "module.transformer.decoder.layers.5.linear1.weight": 524288,
  "module.transformer.decoder.layers.5.linear1.bias": 2048,
  "module.transformer.decoder.layers.5.linear2.weight": 524288,
  "module.transformer.decoder.layers.5.linear2.bias": 256,
  "module.transformer.decoder.layers.5.norm3.weight": 256,
  "module.transformer.decoder.layers.5.norm3.bias": 256,
  "module.transformer.decoder.norm.weight": 256,
  "module.transformer.decoder.norm.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "module.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "module.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "module.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "module.transformer.decoder.bbox_embed.0.layers.0.weight": 65536,
  "module.transformer.decoder.bbox_embed.0.layers.0.bias": 256,
  "module.transformer.decoder.bbox_embed.0.layers.1.weight": 65536,
  "module.transformer.decoder.bbox_embed.0.layers.1.bias": 256,
  "module.transformer.decoder.bbox_embed.0.layers.2.weight": 1024,
  "module.transformer.decoder.bbox_embed.0.layers.2.bias": 4,
  "module.transformer.decoder.class_embed.0.weight": 23296,
  "module.transformer.decoder.class_embed.0.bias": 91,
  "module.transformer.tgt_embed.weight": 230400,
  "module.transformer.enc_output.weight": 65536,
  "module.transformer.enc_output.bias": 256,
  "module.transformer.enc_output_norm.weight": 256,
  "module.transformer.enc_output_norm.bias": 256,
  "module.transformer.enc_out_bbox_embed.layers.0.weight": 65536,
  "module.transformer.enc_out_bbox_embed.layers.0.bias": 256,
  "module.transformer.enc_out_bbox_embed.layers.1.weight": 65536,
  "module.transformer.enc_out_bbox_embed.layers.1.bias": 256,
  "module.transformer.enc_out_bbox_embed.layers.2.weight": 1024,
  "module.transformer.enc_out_bbox_embed.layers.2.bias": 4,
  "module.transformer.enc_out_class_embed.weight": 23296,
  "module.transformer.enc_out_class_embed.bias": 91,
  "module.label_enc.weight": 23552,
  "module.input_proj.0.0.weight": 131072,
  "module.input_proj.0.0.bias": 256,
  "module.input_proj.0.1.weight": 256,
  "module.input_proj.0.1.bias": 256,
  "module.input_proj.1.0.weight": 262144,
  "module.input_proj.1.0.bias": 256,
  "module.input_proj.1.1.weight": 256,
  "module.input_proj.1.1.bias": 256,
  "module.input_proj.2.0.weight": 524288,
  "module.input_proj.2.0.bias": 256,
  "module.input_proj.2.1.weight": 256,
  "module.input_proj.2.1.bias": 256,
  "module.input_proj.3.0.weight": 4718592,
  "module.input_proj.3.0.bias": 256,
  "module.input_proj.3.1.weight": 256,
  "module.input_proj.3.1.bias": 256,
  "module.backbone.0.body.layer2.0.conv1.weight": 32768,
  "module.backbone.0.body.layer2.0.conv2.weight": 147456,
  "module.backbone.0.body.layer2.0.conv3.weight": 65536,
  "module.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "module.backbone.0.body.layer2.1.conv1.weight": 65536,
  "module.backbone.0.body.layer2.1.conv2.weight": 147456,
  "module.backbone.0.body.layer2.1.conv3.weight": 65536,
  "module.backbone.0.body.layer2.2.conv1.weight": 65536,
  "module.backbone.0.body.layer2.2.conv2.weight": 147456,
  "module.backbone.0.body.layer2.2.conv3.weight": 65536,
  "module.backbone.0.body.layer2.3.conv1.weight": 65536,
  "module.backbone.0.body.layer2.3.conv2.weight": 147456,
  "module.backbone.0.body.layer2.3.conv3.weight": 65536,
  "module.backbone.0.body.layer3.0.conv1.weight": 131072,
  "module.backbone.0.body.layer3.0.conv2.weight": 589824,
  "module.backbone.0.body.layer3.0.conv3.weight": 262144,
  "module.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "module.backbone.0.body.layer3.1.conv1.weight": 262144,
  "module.backbone.0.body.layer3.1.conv2.weight": 589824,
  "module.backbone.0.body.layer3.1.conv3.weight": 262144,
  "module.backbone.0.body.layer3.2.conv1.weight": 262144,
  "module.backbone.0.body.layer3.2.conv2.weight": 589824,
  "module.backbone.0.body.layer3.2.conv3.weight": 262144,
  "module.backbone.0.body.layer3.3.conv1.weight": 262144,
  "module.backbone.0.body.layer3.3.conv2.weight": 589824,
  "module.backbone.0.body.layer3.3.conv3.weight": 262144,
  "module.backbone.0.body.layer3.4.conv1.weight": 262144,
  "module.backbone.0.body.layer3.4.conv2.weight": 589824,
  "module.backbone.0.body.layer3.4.conv3.weight": 262144,
  "module.backbone.0.body.layer3.5.conv1.weight": 262144,
  "module.backbone.0.body.layer3.5.conv2.weight": 589824,
  "module.backbone.0.body.layer3.5.conv3.weight": 262144,
  "module.backbone.0.body.layer4.0.conv1.weight": 524288,
  "module.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "module.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "module.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "module.backbone.0.body.layer4.2.conv3.weight": 1048576
}
